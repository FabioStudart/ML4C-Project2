Since the classes are unbalanced, we will calculate ROC-AUC metric to evaluate classifier performance. Okay?
-See Exploratory Data Analysis from part 1 for label histogram.

Due to seaborn version, heatmap figure is bad. Check if later we can use newer python and therefore update seaborn.
-See ClassicMachineLearning from part 1 for heatmap.

Performance of Random Forest and MLP classifiers is extremely high. I wasn't expecting it. I only trained kNN and NB later
to see if the task is easy or these classifiers are really good.

Could not use tsfresh correctly. Will move on to the RNN. -- SOLVED

Dataset object was tested and works well. Realized data is already minmaxed. And in dataset, i do minor alterations in dataset object,
depending on the model. Do unsqueeze so the LSTM accepts my inputs, same for CNN.

ResCNN learns super fast and performs really well! As good as simpleCNN, and maybe better. 

Remember that future papers found layer normalization before attention is better. Will try.  

PART 2, Q1: 
- dataset_q1.py same as from part 1 for CNN but modified labels to long() for CrossEntropy criterion
- train_test_q1.py modifications for multiclass problem in figures, labels squeezing and optimizer (CrossEntropy)
- model_q1.py modify last layer to get output size of 5

TO DO (part 1):
Implement simple ML models with engineered features
Train LSTM and BidirectionalLSTM again
in CNN, ResCNN, maybe try dropout to reduce overfitting. However, there is no great evidence of overfitting, as validation error doesn't increase
(questions need to be discussed)

TRAIN EVERYTHING JUST FOR 100/200 EPOCHS
Train ResCNN and simpleCNN only for 100/200 epochs, to conclude that ResCNN is faster to train.

APPLY TRANSFORMER ENCODER BLOCK DIRECTLY ON DATA SO WE CAN SEE ACTIVATION MAPS.

TO DO (part 2):
All
