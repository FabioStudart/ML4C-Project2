Since the classes are unbalanced, we will calculate ROC-AUC metric to evaluate classifier performance. Okay?
-See Exploratory Data Analysis from part 1 for label histogram.

Due to seaborn version, heatmap figure is bad. Check if later we can use newer python and therefore update seaborn.
-See ClassicMachineLearning from part 1 for heatmap.

Performance of Random Forest and MLP classifiers is extremely high. I wasn't expecting it. I only trained kNN and NB later
to see if the task is easy or these classifiers are really good.

Could not use tsfresh correctly. Will move on to the RNN.

Dataset object was tested and works well. Realized data is already minmaxed. And in dataset, i do minor alterations in dataset object,
depending on the model. Do unsqueeze so the LSTM accepts my inputs, same for CNN.

RNN training and testing is implemented. However, output of LSTM is the same for all samples in validation set. I believe it is overfitting the majority class.
When I implement the dataloader that is balanced, it just predicts 0 for all.

Implemented Bidirectional LSTM. Same problem.

Started with simple CNN. Looks like im having the same problem.

After training first simpleCNN and then simplererCNN, realized LSTM might be too simple to differentiate between classes.

HAD PROBLEMS WITH ACCURACY COMPUTATION. Now the training is going better for the CNN. Reorganized the code, with a ModelZoo, training file and evaluation file.

If try to show plt figures in terminal, program crashes and cannot save them.

simpleCNN is overfitting. Somehow good on val, bad on train.

TO DO (part 1):
Implement simple ML models with engineered features
Train LSTM and BidirectionalLSTM again
simpleCNN is overfitting. Somehow good on val, bad on train.
Implement ResCNN
Implement Tranformer

(questions need to be discussed)

TO DO (part 2):
All